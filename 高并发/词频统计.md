# 词频统计

## 要求

- 一个1G大小的文件，文件中每一行是一个单词，单词大小不超过32字节。
- 单机，内存限制10MB。
- 计算出出现频次最高的50个单词。

## 分析

首先，内存有限制，10MB的内存是无论如何都不能加载1G大小的文件，因此文件需要一部分一部分的处理，但是要统计每个单词的出现频次，又必须要将其记录下来，而内存又不够。之所以必须要将其记录下来，是因为相同的单词基本上一定会分布在不同的文件里，当处理完一个文件之后，在处理下一个文件时，因为内存不足，因而必须将上一个文件的处理统计结果丢弃。但如果在对文件进行拆分的时候，将相同的单词分布在同一个文件中；或者对单词进行排序，使相同的单词挨在一起，然后顺序读取统计（读完之后就丢弃，因为只需要统计其出现频次而已）而已，如此就可以解决问题。然后始终只保留50个当前已出现频次最多的单词，当来了新的单词时，看是否大于50个单词中出现频次最小的单词，如果大于，就丢弃最小频次的单词，并将新的单词添加到这50个单词的集合中。

## 思路

- 哈希分片法
  1. 根据内存大小进行文件分片。在进行文件分片时，通过对单词进行hash计算，将相同单词分片到相同的文件中。
  2. 计算每个文件出现单词的词频，并记录下来。
  3. 维护一个大小为50的小根堆，将第二步计算的词频加入到堆中。然后重复二三步，直到所有分片文件计算完成，最后堆中剩下的50个单词就是出现频次对多的单词。
- 归并排序法
  1. 通过归并排序算法对文件中的单词进行归并排序。
  2. 每次归并的中间结果通过文件存储。
  3. 维护一个大小为50的小根堆，读取最终归并的文件，依次将每个单词的出现次数统计出来，并加入到小根堆中。

## 生成单词文件

为了便于测试，通过代码生成符合要求的单词文件。

```java
/**
  * 生成用于测试的单词文件的大小，单位MB。
  * 如果指定100MB，则生成127MB大小的文件。
  * 如果指定1024MB，则生成1.27GB大小的文件。
  */
private static final int FILE_SIZE = 100;

/**
  * 生成用于测试的单词文件
  * char : 16位 —— 两个字节
  */
public static void generateWordFile(String fileName) throws IOException {
    Random random = new Random();
    char[] letter = new char[]{
        'a', 'b', 'c', 'd', 'e', 'f', 'g',
        'h', 'l', 'j', 'k', 'l', 'm', 'n',
        'o', 'p', 'q', 'r', 's', 't',
        'u', 'v', 'w', 'x', 'y', 'z'};
    try (Writer writer = new FileWriter(fileName)) {
        int oneM = 1048576; //1024 * 1024;
        int m = 0;
        for (int i = 0; i < FILE_SIZE; i++) {
            for (; ; ) {
                int len = random.nextInt(8);
                if (len == 0) len = 1;
                if ((m += len) >= oneM) {
                    writer.flush();
                    m = 0;
                    break;
                }
                for (int j = 0; j < len; j++)
                    writer.append(letter[random.nextInt(26)]);
                writer.append("\n");
            }
        }
    }
}
```

## 单词实体

```java
@Data
@AllArgsConstructor
public class Word {

    private String word;

    private int count;
    
    @Override
    public String toString() {
        return "单词'"+word+"'出现次数："+count;
    }
}
```

## 小根堆

```java
public class WordHeap {

    private int heapIndex = 0;

    /**
     * 堆结点总数
     */
    private final int heapSize;

    private final int biggestIndex;

    @Getter
    private final Word[] heap;

    public WordHeap(int heapSize) {
        this.heapSize = heapSize;
        this.biggestIndex = heapSize - 1;
        this.heap = new Word[heapSize];
    }

    /**
     * 构建堆
     **/
    private void buildHeap() {
        int last = heapSize / 2 - 1;
        for (int i = last; i >= 0; i--) {
            heapAdjust(i);
        }
    }

    /**
     * 调整堆
     *
     * @param i  出发结点索引
     */
    private void heapAdjust(int i) {
        while (i <= heapSize / 2 - 1) {
            //i的左右叶子结点索引
            int l = i * 2 + 1, r = l + 1;
            //i要交换的叶子结点索引
            int j = r < heapSize && heap[l].getCount() > heap[r].getCount() ? r : l;
            //i结点小于j结点（小根堆），无须交换，退出
            if (heap[i].getCount() < heap[j].getCount()) break;
            //swap
            int temp = heap[i].getCount();
            heap[i].setCount(heap[j].getCount());
            heap[j].setCount(temp);
            //递进出发结点索引
            i = j;
        }
    }

    public void intoHeap(String word, int count){
        if (heapIndex <= biggestIndex) {
            // 将堆填满，当堆满时，构建堆
            heap[heapIndex] = new Word(word, count);
            if (heapIndex++ == biggestIndex)
                buildHeap(); // 构建堆
        } else {
            // 当前单词数量大于堆顶（小根堆）单词的数量，则替换，并调整堆
            if (count > heap[0].getCount()) {
                heap[0] = new Word(word, count);
                heapAdjust(0); //调整堆，从堆顶出发
            }
        }
    }
}
```

## 哈希分片法

文件分片

```java
/**
  * 文件分片
  * <p>
  * 因为内存有限，所以需要对原始文件进行拆分。通过hash将相同的单词拆分到同一个文件。
  * 考虑到虽然内存有限，但相同的单词只需要统计总数，因此每个文件的大小是不用限制的，
  * 而是以不同单词数量划分（不同单词数量太多，在每次读取到hashMap中进行数量统计时，
  * 可能内存溢出）。但是再考虑到极端情况，即原始文件中没有一个单词是相同的，所以可
  * 直接按照可用内存大小和文件大小进行划分而得出子文件数量。为了单词分布尽可能平均，
  * hash计算借用HashMap的计算方式，子文件数量为2的N次幂。
  *
  * @param fileName 原始文件
  * @param memoryForMb 预期每个分片文件的大小，单位MB
  * @return 分片文件。
  */
public static List<File> fileFragmentation(String fileName, long memoryForMb) throws IOException {
    File file = new File(fileName);
    String[] names = file.getName().split("\\.");
    String prefix = names[0], suffix = names[1];
    String rootPath = file.getParent() + File.separator + prefix;
    //子文件集合
    List<File> childFiles = new LinkedList<>();
    //计算分片数量
    int num = fragmentNumber(file, memoryForMb);
    //子文件输出流集合
    PrintWriter[] pws = new PrintWriter[num];
    //子文件输出大小统计
    long[] size = new long[num];
    //1MB
    int oneM = 1024 * 1024;
    try (BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(file)))) {
        String word;
        while ((word = br.readLine()) != null) {
            //计算hash位
            int index = (num - 1) & hash(word);
            PrintWriter writer;
            if (pws[index] == null) {
                //子文件名称
                String childFileName = rootPath + "_" + index + "." + suffix;
                //创建文件
                File childFile = new File(childFileName);
                if (!childFile.exists() && childFile.createNewFile())
                    childFiles.add(childFile);
                writer = pws[index] = new PrintWriter(new FileWriter(childFile, true));
            } else {
                writer = pws[index];
            }
            assert writer != null;
            writer.append(word).append("\n");
            //统计每个文件的大小，当超过1M时，刷新到文件中
            if (size[index] >= oneM) {
                writer.flush(); //批量刷新流，如果写一个刷一次，效率将非常慢
                size[index] = 0;
            } else {
                size[index] += word.length();
            }
        }
    }
    Arrays.stream(pws).forEach(PrintWriter::close);
    return childFiles;
}

/**
  * 计算文件分片数量。
  *
  * @param file 原始文件
  * @param memoryForMb 预期每个分片文件的大小，单位MB
  * @return 文件分片数量
  */
static int fragmentNumber(File file, long memoryForMb) {
    long length = file.length();
    return tableSizeFor((int) (length / (memoryForMb * 1024 * 1024)));
}

/**
  * 引用HashMap中的数组大小计算方式，大小始终为2的N次幂。对应子文件的数量。
  */
static int tableSizeFor(int cap) {
    int n = -1 >>> Integer.numberOfLeadingZeros(cap - 1);
    return (n < 0) ? 1 : n + 1;
}

/**
  * 用HashMap中的key的hash计算方式。
  */
static int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

综合操作

切割文件，读取子文件，入堆排序

```java
/**
  * 统计文件中每个单词出现的次数
  * @param file 文件。
  * @return 单词->频次
  */
private static Map<String, Integer> wordFrequency(File file) throws Exception{
    Map<String, Integer> countMap = new HashMap<>();
    try (BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(file)))) {
        String line;
        while ((line = br.readLine()) != null) {
            if (countMap.containsKey(line))
                countMap.put(line, countMap.get(line) + 1);
            else
                countMap.put(line, 1);
        }
    }
    return countMap;
}

/**
  * -Xmx1000M
  *
  * @param fileName 文件名称
  */
public static Word[] readerFile(String fileName) throws Exception {
    // 文件分片，将一半的可用内存用来计算分片数量
    List<File> files = fileFragmentation(fileName, 1); //定义每个文件的大小尽可能在一MB左右
    // word heap （小根堆）
    WordHeap heap = new WordHeap(50);
    // 迭代读取所有子文件
    for (File file : files) {
        wordFrequency(file) //统计当前子文件中每个单词出现的次数
            .forEach(heap::intoHeap);//堆处理
    }
    return heap.getHeap();
}
```

## 归并排序法



单词文件归并排序

```java
public class WordFrequencyStatistics {

    private String rootPath;

    private int i;

    public File mergeSort(String fileName) throws Exception {
        return mergeSort(new File(fileName));
    }

    public File mergeSort(File file) throws Exception {
        this.rootPath = file.getParent();
        this.i = 0;
        int lineNumber = getFileLineNumber(file);
        try (BufferedReader sourceReader = new BufferedReader(new InputStreamReader(new FileInputStream(file)))) {
            return mergeSort(sourceReader, 0, lineNumber - 1);
        }
    }

    /**
     * @param sourceReader 归并源文件的输入流。
     * @param pos          起始行偏移量
     * @param end          结束行偏移量
     * @return 归并文件
     */
    private File mergeSort(BufferedReader sourceReader, int pos, int end) throws Exception {
        if (pos < end - 1) {
            int center = (end + pos) / 2;
            File leftFile = mergeSort(sourceReader, pos, center);
            File rightFile = mergeSort(sourceReader, center + 1, end);
            return merge(leftFile, rightFile);
        } else if (end - pos == 1) {
            String l = sourceReader.readLine();
            String r = sourceReader.readLine();

            File transferFile = createFile();
            try (BufferedWriter transferWriter = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(transferFile)))) {
                if (l.compareTo(r) < 0) {
                    transferWriter.append(l).append("\n").append(r).append("\n");
                } else {
                    transferWriter.append(r).append("\n").append(l).append("\n");
                }
                transferWriter.flush();
            }
            return transferFile;
        } else {
            String l = sourceReader.readLine();
            File transferFile = createFile();
            try (BufferedWriter transferWriter = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(transferFile)))) {
                transferWriter.append(l).append("\n");
                transferWriter.flush();
            }
            return transferFile;
        }
    }

    private File createFile() throws IOException {
        File file = new File(rootPath + File.separator + "transfer-" + (i++) + ".txt");
        if (file.createNewFile()) {
            System.out.println("文件" + file.getName() + "创建成功");
        }
        return file;
    }

    private void deleteFile(File file) {
        if (file.delete()) {
            System.out.println("文件" + file.getName() + "删除成功");
        }
    }

    /**
     * 获取文件的行数
     */
    public int getFileLineNumber(File file) throws IOException {
        try (BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(file)))) {
            int lineNumber = 0;
            while (reader.readLine() != null)
                lineNumber++;
            return lineNumber;
        }
    }

    /**
     * 两个有序文件归并。
     *
     * @param leftFile 有序文件1
     * @param rightFile 有序文件2
     * @return 归并之后的有序文件3
     */
    private File merge(File leftFile, File rightFile) throws Exception {
        File transferFile;
        try (BufferedReader leftReader = new BufferedReader(new InputStreamReader(new FileInputStream(leftFile)));
             BufferedReader rightReader = new BufferedReader(new InputStreamReader(new FileInputStream(rightFile)));
             BufferedWriter transferWriter = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(transferFile = createFile())))) {

            String leftLine = null, rightLine = null;
            //左文件和右文件读取的数据都不等于NULL，则表示都没有读取完，while循环继续读取
            while ((leftLine == null ? (leftLine = leftReader.readLine()) : leftLine) != null
                    && (rightLine == null ? (rightLine = rightReader.readLine()) : rightFile) != null) {
                if (leftLine.compareTo(rightLine) < 0) {
                    transferWriter.append(leftLine);
                    leftLine = null;
                } else {
                    transferWriter.append(rightLine);
                    rightLine = null;
                }
                transferWriter.append("\n");
            }
            transferWriter.flush();

            while ((leftLine == null ? (leftLine = leftReader.readLine()) : leftLine) != null) {
                transferWriter.append(leftLine).append("\n");
                leftLine = null;
            }
            transferWriter.flush();

            while ((rightLine == null ? (rightLine = rightReader.readLine()) : rightFile) != null) {
                transferWriter.append(rightLine).append("\n");
                rightLine = null;
            }
            transferWriter.flush();
        }
        //删除中转文件，注意，必须先关闭相关IO流，否则文件不能被删除
        deleteFile(leftFile);
        deleteFile(rightFile);
        return transferFile;
    }
}
```

综合操作

归并文件，读取归并排序之后的文件，依次读取入堆排序。

```java
public Word[] readerFile(String fileName) throws Exception {
    // 文件分片，将一半的可用内存用来计算分片数量
    File file = mergeSort(fileName);
    // word heap （小根堆）
    WordHeap heap = new WordHeap(50);

    try (BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(file)))) {
        String lastWord = null, currentWord;
        int count = 1;
        while ((currentWord = reader.readLine()) != null) {
            if (lastWord == null) {
                lastWord = currentWord;
            } else if (lastWord.equals(currentWord)) {
                count++;
            } else {
                heap.intoHeap(lastWord, count);
                lastWord = currentWord;
                count = 1;
            }
        }
        heap.intoHeap(lastWord, count);
    }
    deleteFile(file);
    return heap.getHeap();
}

public static void main(String[] args) throws Exception {
    String fileName = "C:\\Users\\xlp\\Desktop\\1MERGE\\1M.txt";
    WordFrequencyStatistics wfs = new WordFrequencyStatistics();
    Word[] wh = wfs.readerFile(fileName);
    Arrays.stream(wh)
        .sorted(Comparator.comparingInt(Word::getCount))
        .forEach(System.out::println);
}
```



上述的对文件的归并操作执行效率有点慢，即使是一个只有一MB的文件，也要计算许久，但不管怎样，至少都证明归并排序是可行的，剩下的只是效率的优化问题。

效率之所以慢是因为每次中转文件都要新建和删除，并且需要打开和关闭IO流，当这个操作需要执行上万次时，效率自然很慢。

有两个点可以优化：

1. 文件只在初始的时候新建，并打开IO流，之后的每次中转记录用同一个文件，每次中转时覆盖原来的内容即可。
2. 可以先将文件中的单词以一万个为一组先行在内存中排序，然后在以一万个为一组进行归并排序，而不是归并操作递归到最底层以两个单词排序。这样效率至少可以提升一万倍。







